<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Fandongmeng.GitHub.com : My online resume">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Fandong Meng's Homepage</title>
    <meta name ="keywords" content="Fandong Meng, Meng Fandong, fandong meng, meng fandong, mengfandong">
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/fandongmeng">View on GitHub</a>

          <h1 id="project_title">Fandong Meng (孟凡东)</h1>
          <!--<h2 id="project_tagline">My online resume</h2>-->

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

<h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Personal Information</h2>

<td class="wsite-multicol-col">
    <img border="0" margin="0 0 0 0" height="192" width="160" src="pics/mfd-macao.jpeg" align="left">
</td>
        
<td style="width:10pt"></td>
<td align="left">

<p>He is a principal researcher and team leader at WeChat, Tencent Inc, China. </p> 
<p>He got his Ph.D. degree at Institute of Computing Technology, Chinese Academy of Sciences. </p> 
<p>His research interests include machine translation, natural language processing and large language modeling. </p> <br> <br>
<p>E-mail: fandongmeng [at] tencent [dot] com</p>
</td> 

<h2><a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>News</h2> 
<li type="circle"> 02/05/2024: Two papers were accepted at ICML 2024. </li> 
<li type="circle"> 13/03/2024: Two papers were accepted at NAACL 2024. </li> 
<li type="circle"> 20/02/2024: Two papers were accepted at LREC-COLING 2024. </li> 
<li type="circle"> 16/01/2024: Two papers were accepted at ICLR 2024. </li> 
<li type="circle"> 09/12/2023: Three papers were accepted at AAAI 2024. </li> 


<h2><a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h2>      
<div id="google-scholar"> Please go to <a href="https://fandongmeng.github.io/publications.html">[Full List]</a> or <a href="https://scholar.google.com/citations?user=sA8U4S0AAAAJ&hl=en">[Google Scholar]</a> to see all my publications. </div> 

<h3> Recent Preprints </h3>
<ul> 
<li type="circle">
Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, <strong>Fandong Meng</strong>, Jie Zhou, Ju Ren and Yaoxue Zhang.
<font color="#0080FF">ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning.</font>
[<a href="https://arxiv.org/abs/2505.04881">arxiv</a>]  
</li>
<li type="circle">
Xue Zhang, Songming Zhang, Yunlong Liang, <strong>Fandong Meng</strong>, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">A Dual-Space Framework for General Knowledge Distillation of Large Language Models.</font>
[<a href="https://arxiv.org/abs/2504.11426">arxiv</a>]  
</li>  
<li type="circle">
Jiaan Wang, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">Deep Reasoning Translation via Reinforcement Learning.</font>
[<a href="https://arxiv.org/abs/2504.10187">arxiv</a>]  
</li>
<li type="circle">
Zhibin Lan, Liqiang Niu, <strong>Fandong Meng</strong>, Jie Zhou and Jinsong Su.
<font color="#0080FF">LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning.</font>
[<a href="https://arxiv.org/abs/2503.04812">arxiv</a>]  
</li>
<li type="circle">
Xinyan Guan, Jiali Zeng, <strong>Fandong Meng</strong>, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun and Jie Zhou. 
<font color="#0080FF">DeepRAG: Thinking to Retrieval Step by Step for Large Language Models.</font>
[<a href="https://arxiv.org/abs/2502.01142">arxiv</a>]  
</li>
<li type="circle">
Jiaan Wang, <strong>Fandong Meng</strong>, Yingxue Zhang and Jie Zhou.
<font color="#0080FF">Retrieval-Augmented Machine Translation with Unstructured Knowledge.</font>
[<a href="https://arxiv.org/abs/2412.04342">arxiv</a>]  
</li>
<li type="circle">
Meiqi Chen, <strong>Fandong Meng</strong>, Yingxue Zhang, Yan Zhang and Jie Zhou.
<font color="#0080FF">CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models.</font>
[<a href="https://arxiv.org/abs/2410.21067">arxiv</a>]  
</li>
<li type="circle">
Xiangyu Hong, Che Jiang, Biqing Qi, <strong>Fandong Meng</strong>, Mo Yu, Bowen Zhou and Jie Zhou.
<font color="#0080FF">On the token distance modeling ability of higher RoPE attention dimension.</font>
[<a href="https://arxiv.org/abs/2410.08703">arxiv</a>]  
</li>
<li type="circle">
Chao Hu, Yitian Chai, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Xiaodong Gu.
<font color="#0080FF">How Effectively Do Code Language Models Understand Poor-Readability Code?.</font>
[<a href="https://guxd.github.io/papers/hu2024ase-poorcodesumeval.pdf">paper</a>]  
</li>
<li type="circle">
Wenchao Chen, Liqiang Niu, Ziyao Lu, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">MaskMamba: A Hybrid Mamba-Transformer Model for Masked Image Generation.</font>
[<a href="https://arxiv.org/abs/2409.19937">arxiv</a>]  
</li>
<li type="circle">
Zhen Yang, Yingxue Zhang, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models.</font>
[<a href="https://arxiv.org/abs/2311.04589">arxiv</a>]  
</li>
<li type="circle">
Yun Luo, Zhen Yang, <strong>Fandong Meng</strong>, Yafu Li, Jie Zhou and Yue Zhang.
<font color="#0080FF">An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning.</font>
[<a href="https://arxiv.org/abs/2308.08747">arxiv</a>]  
</li>
<li type="circle">
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Xu Sun.
<font color="#0080FF">RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge.</font>
[<a href="https://arxiv.org/abs/2311.08147">arxiv</a>]  
</li>
<li type="circle">
Yijie Chen, Yijin Liu, <strong>Fandong Meng</strong>, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">Improving Translation Faithfulness of Large Language Models via Augmenting Instructions.</font>
[<a href="https://arxiv.org/abs/2308.12674">arxiv</a>]  

</ul> 
        
<h3> Recent Publications </h3>
<ul>

<li type="circle">
Yunlong Liang, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation.</font>
In <em> Proceedings of ACL 2025</em>.
</li>
<li type="circle">
Jiaan Wang, <strong>Fandong Meng</strong>, Zengkui Sun, Yunlong Liang, Yuxuan Cao, Jiarong Xu, Haoxiang Shi and Jie Zhou.
<font color="#0080FF">An Empirical Study of Many-to-Many Summarization with Large Language Models.</font>
In <em> Proceedings of ACL 2025</em>.
</li>
<li type="circle">
Xue Zhang, Yunlong Liang, <strong>Fandong Meng</strong>, Songming Zhang, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts.</font>
In <em> Proceedings of ACL 2025</em>.
</li>
<li type="circle">
Liang Zhang, Ziyao Lu, <strong>Fandong Meng</strong>, Hui Li, Jie Zhou and Jinsong Su.
<font color="#0080FF">Advancing SMoE for Continuous Domain Adaptation of MLLMs: Adaptive Router and Domain-Specific Loss.</font>
In <em> Proceedings of ACL 2025</em>.
</li>
<li type="circle">
Liang Zhang, Yang Zhang, Ziyao Lu, <strong>Fandong Meng</strong>,, Jie Zhou and Jinsong Su.
<font color="#0080FF">A Self-Denoising Model for Robust Few-Shot Relation Extraction.</font>
In <em> Proceedings of ACL 2025</em>.
</li>
<li type="circle">
Kun Ouyang, Yuanxin Liu, Shicheng Li, Yi Liu, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Xu Sun.
<font color="#0080FF">PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension.</font>
In <em> Proceedings of ACL 2025</em>.
</li>
<li type="circle">
Jiaan Wang, <strong>Fandong Meng</strong>, Yunlong Liang and Jie Zhou.
<font color="#0080FF">DRT: Deep Reasoning Translation via Long Chain-of-Thought.</font>
In <em> Findings of ACL 2025</em>.
</li>
<li type="circle">
Bowen Ping, Jiali Zeng, <strong>Fandong Meng</strong>, Shuo Wang, Jie Zhou and Shanghang Zhang.
<font color="#0080FF">LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information.</font>
In <em> Findings of ACL 2025</em>.
</li>
<li type="circle">
Yijie Chen, Yijin Liu, <strong>Fandong Meng</strong>, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">Enhancing Cross-Tokenizer Knowledge Distillation with Contextual Dynamical Mapping.</font>
In <em> Findings of ACL 2025</em>.
</li>
<li type="circle">
Zhibin Lan, Liqiang Niu, <strong>Fandong Meng</strong>, Wenbo Li, Jie Zhou and Jinsong Su.
<font color="#0080FF">AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity.</font>
In <em> Findings of ACL 2025</em>.
</li>
<li type="circle">
Jiaxin Shen, Jinan Xu, Huiqi Hu, Luyi Lin, Guoyang Ma, Fei Zheng, <strong>Fandong Meng</strong>,, Jie Zhou and Wenjuan Han.
<font color="#0080FF">A Law Reasoning Benchmark for LLM with Tree-Organized Structures including Factum Probandum, Evidence and Experiences.</font>
In <em> Findings of ACL 2025</em>.
</li>
<li type="circle">
Chenze Shao, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">Continuous Visual Autoregressive Generation via Score Maximization.</font>
In <em> Proceedings of ICML 2025</em>.
</li>
<li type="circle">
Chenze Shao, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">Patch-Level Training for Large Language Models.</font>
In <em> Proceedings of ICLR 2025</em>.
[<a href="https://arxiv.org/abs/2407.12665">arxiv</a>]  
</li>
<li type="circle">
Yuxian Gu, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Minlie Huang.
<font color="#0080FF">MiniPLM: Knowledge Distillation for Pre-Training Language Models.</font>
In <em> Proceedings of ICLR 2025</em>.
[<a href="https://arxiv.org/abs/2410.17215">arxiv</a>]  
</li>
<li type="circle">
Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, <strong>Fandong Meng</strong>, Jie Zhou and Min Zhang.
<font color="#0080FF">DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory.</font>
In <em> Proceedings of ICLR 2025</em>.
[<a href="https://arxiv.org/abs/2410.08143">arxiv</a>]  
</li>
<li type="circle">
Yucheng Ding, Yangwenjian Tan, Xiangyu Liu, Chaoyue Niu, <strong>Fandong Meng</strong>, Jie Zhou, Ning Liu, Fan Wu and Guihai Chen.
<font color="#0080FF">Personalized Language Model Learning on Text Data Without User Identifiers.</font>
In <em> Proceedings of KDD 2025</em>.
</li>
<li type="circle">
Xue Zhang, Yunlong Liang, <strong>Fandong Meng</strong>, Songming Zhang, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">Multilingual Knowledge Editing with Language-Agnostic Factual Neurons.</font>
In <em> Proceedings of COLING 2025</em>.
[<a href="https://arxiv.org/abs/2406.16416">arxiv</a>]  
</li>
</ul> 

        
<h2>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Services</h2>
<ul>
<li type="circle">Action Editor: ACL Rolling Review (2024)</li>
<li type="circle">Standing Reviewer: TACL (2021-)</li>
<li type="circle">Meta-Reviewer: NeurIPS 2024 (Area Chair); AAAI 2022 (Senior PC Member) </li>
<li type="circle">PC Member & Reviewer: NeurIPS (2023); ACL (2014, 2017-2022); EMNLP (2018, 2020-2022); NAACL (2016, 2018, 2021); AAAI (2020, 2021); IJCAI (2019); COLING (2018) </li>
</ul>

<h2>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honours</h2>
<ul>
<li type="circle"><font color=red>Best Long Paper Awards</font> of ACL 2019 and EMNLP 2023. </li> 
<li type="circle">Champion of ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents. </li>
<li type="circle">Champion of WMT22 Chat Translation Task on English->German and German->English. </li>
<li type="circle">Champion of WMT22 Biomedical Translation Task on Chinese->English. </li>  
<li type="circle">Champion of WMT21 news translation task on English->Chinese, English->Japanese, Japanese->English, and English->German (among constrained systems). </li>  
<li type="circle">Champion of WMT20 news translation task on Chinese->English. </li> 
<li type="circle">National Scholarship for Graduate Excellence of UCAS in 2015. </li>
<li type="circle">UCAS Outstanding Student Awards in 2013 and 2014. </li>
<li type="circle">Outstanding Winner (<i>world 1/14</i>) of COMAP's Mathematical Contest in Modeling (MCM/<u>ICM</u>) in 2010. </li>
<li type="circle">First Prize Winner of National Mathematic Contest In Modeling in 2009. </li>
</ul>

      </section>

  </body>
</html>
