<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Fandongmeng.GitHub.com : My online resume">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Fandong Meng's Homepage</title>
    <meta name ="keywords" content="Fandong Meng, Meng Fandong, fandong meng, meng fandong, mengfandong">
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/fandongmeng">View on GitHub</a>

          <h1 id="project_title">Fandong Meng (孟凡东)</h1>
          <!--<h2 id="project_tagline">My online resume</h2>-->

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

<h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Personal Information</h2>

<td class="wsite-multicol-col">
    <img border="0" margin="0 0 0 0" height="192" width="160" src="pics/mfd-macao.jpeg" align="left">
</td>
        
<td style="width:10pt"></td>
<td align="left">

<p>He is a principal researcher and team leader at WeChat, Tencent Inc, China. </p> 
<p>He got his Ph.D. degree at Institute of Computing Technology, Chinese Academy of Sciences. </p> 
<p>His research interests include machine translation, natural language processing and large language modeling. </p> <br> <br>
<p>E-mail: fandongmeng [at] tencent [dot] com</p>
</td> 

<h2><a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>News</h2> 
<li type="circle"> 02/05/2024: Two papers were accepted at ICML 2024. </li> 
<li type="circle"> 13/03/2024: Two papers were accepted at NAACL 2024. </li> 
<li type="circle"> 20/02/2024: Two papers were accepted at LREC-COLING 2024. </li> 
<li type="circle"> 16/01/2024: Two papers were accepted at ICLR 2024. </li> 
<li type="circle"> 09/12/2023: Three papers were accepted at AAAI 2024. </li> 


<h2><a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h2>      
<div id="google-scholar"> Please go to <a href="https://fandongmeng.github.io/publications.html">[Full List]</a> or <a href="https://scholar.google.com/citations?user=sA8U4S0AAAAJ&hl=en">[Google Scholar]</a> to see all my publications. </div> 

<h3> Recent Preprints </h3>
<ul> 

<li type="circle">
Meiqi Chen, <strong>Fandong Meng</strong>, Yingxue Zhang, Yan Zhang and Jie Zhou.
<font color="#0080FF">CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models.</font>
[<a href="https://arxiv.org/abs/2410.21067">arxiv</a>]  
</li>
<li type="circle">
Yuxian Gu, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Minlie Huang.
<font color="#0080FF">MiniPLM: Knowledge Distillation for Pre-Training Language Models.</font>
[<a href="https://arxiv.org/abs/2410.17215">arxiv</a>]  
</li>
<li type="circle">
Xiangyu Hong, Che Jiang, Biqing Qi, <strong>Fandong Meng</strong>, Mo Yu, Bowen Zhou and Jie Zhou.
<font color="#0080FF">On the token distance modeling ability of higher RoPE attention dimension.</font>
[<a href="https://arxiv.org/abs/2410.08703">arxiv</a>]  
</li>
<li type="circle">
Chao Hu, Yitian Chai, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou, Xiaodong Gu.
<font color="#0080FF">How Effectively Do Code Language Models Understand Poor-Readability Code?.</font>
[<a href="https://guxd.github.io/papers/hu2024ase-poorcodesumeval.pdf">paper</a>]  
</li>
<li type="circle">
Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, <strong>Fandong Meng</strong>, Jie Zhou and Min Zhang.
<font color="#0080FF">DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory.</font>
[<a href="https://arxiv.org/abs/2410.08143">arxiv</a>]  
</li>
<li type="circle">
Zhibin Lan, Liqiang Niu, <strong>Fandong Meng</strong>, Wenbo Li, Jie Zhou and Jinsong Su.
<font color="#0080FF">AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity.</font>
[<a href="https://arxiv.org/abs/2410.02745">arxiv</a>]  
</li>
<li type="circle">
Wenchao Chen, Liqiang Niu, Ziyao Lu, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">MaskMamba: A Hybrid Mamba-Transformer Model for Masked Image Generation.</font>
[<a href="https://arxiv.org/abs/2409.19937">arxiv</a>]  
</li>
<li type="circle">
Chenze Shao, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">Patch-Level Training for Large Language Models.</font>
[<a href="https://arxiv.org/abs/2407.12665">arxiv</a>]  
</li>
<li type="circle">
Xue Zhang, Yunlong Liang, <strong>Fandong Meng</strong>, Songming Zhang, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">Multilingual Knowledge Editing with Language-Agnostic Factual Neurons.</font>
[<a href="https://arxiv.org/abs/2406.16416">arxiv</a>]  
</li>
<li type="circle">
Zhen Yang, Yingxue Zhang, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models.</font>
[<a href="https://arxiv.org/abs/2311.04589">arxiv</a>]  
</li>
<li type="circle">
Yun Luo, Zhen Yang, <strong>Fandong Meng</strong>, Yafu Li, Jie Zhou and Yue Zhang.
<font color="#0080FF">An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning.</font>
[<a href="https://arxiv.org/abs/2308.08747">arxiv</a>]  
</li>
<li type="circle">
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Xu Sun.
<font color="#0080FF">RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge.</font>
[<a href="https://arxiv.org/abs/2311.08147">arxiv</a>]  
</li>
<li type="circle">
Yijie Chen, Yijin Liu, <strong>Fandong Meng</strong>, Yufeng Chen, Jinan Xu and Jie Zhou.
<font color="#0080FF">Improving Translation Faithfulness of Large Language Models via Augmenting Instructions.</font>
[<a href="https://arxiv.org/abs/2308.12674">arxiv</a>]  

</ul> 
        
<h3> Recent Publications </h3>
<ul>
<li type="circle">
Kunting Li, Yong Hu, Liang He, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">C-LLM: Learn to Check Chinese Spelling Errors Character by Character.</font>
In <em> Proceedings of EMNLP 2024</em>.
[<a href="https://arxiv.org/abs/2406.16536">arxiv</a>]  
</li>
<li type="circle">
Liang Zhang, Zhen Yang, Biao Fu, Ziyao Lu, Liangying Shao, Shiyu Liu, <strong>Fandong Meng</strong>, Jie Zhou, Xiaoli Wang and Jinsong Su.
<font color="#0080FF">Multi-Level Cross-Modal Alignment for Speech Relation Extraction.</font>
In <em> Proceedings of EMNLP 2024</em>.
</li>
<li type="circle">
Chenze Shao, <strong>Fandong Meng</strong>, Yijin Liu and Jie Zhou.
<font color="#0080FF">Language Generation with Strictly Proper Scoring Rules.</font>
In <em> Proceedings of ICML 2024</em>.
[<a href="https://arxiv.org/abs/2405.18906">arxiv</a>]
</li>
<li type="circle">
Chujie Zheng, Fan Yin, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou, Kai-Wei Chang, Minlie Huang and Nanyun Peng.
<font color="#0080FF">On Prompt-Driven Safeguarding for Large Language Models.</font>
In <em> Proceedings of ICML 2024</em>.
</li>
<li type="circle">
Chenze Shao, <strong>Fandong Meng</strong>, Jiali Zeng and Jie Zhou.
<font color="#0080FF">Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective.</font>
In <em> Proceedings of ACL 2024</em>.
[<a href="https://arxiv.org/abs/2405.18922">arxiv</a>]
</li>
<li type="circle">
Yunlong Liang, <strong>Fandong Meng</strong>, Jiaan Wang, Jinan Xu, Yufeng Chen and Jie Zhou.
<font color="#0080FF">Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation.</font>
In <em> Proceedings of ACL 2024</em>.
</li>
<li type="circle">
Yong Hu, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers.</font>
In <em> Proceedings of ACL 2024</em>.
</li>
<li type="circle">
Yutong Wang, Jiali Zeng, Xuebo Liu, <strong>Fandong Meng</strong>, Jie Zhou and Min Zhang.
<font color="#0080FF">TasTe: Teaching Large Language Models to Translate through Self-Reflection.</font>
In <em> Proceedings of ACL 2024</em>.
</li>
<li type="circle">
Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu and <strong>Fandong Meng</strong>. 
<font color="#0080FF">Cross-Lingual Knowledge Editing in Large Language Models.</font>
In <em> Proceedings of ACL 2024</em>.
</li>
<li type="circle">
Shicheng Xu, Liang Pang, Mo Yu, <strong>Fandong Meng</strong>, Huawei Shen, Xueqi Cheng and Jie Zhou.
<font color="#0080FF">Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation.</font>
In <em> Proceedings of ACL 2024</em>.
</li>
<li type="circle">
Bin Sun, Jianfeng Li, Hao Zhou, <strong>Fandong Meng</strong>, Kan Li, Jie Zhou.
<font color="#0080FF">Exploring Conditional Variational Mechanism to Pinyin Input Method for Addressing One-to-Many Mappings in Low-Resource Scenarios.</font>
In <em> Proceedings of ACL 2024 (short)</em>.
</li>
<li type="circle">
Yun Luo, Zhen Yang, <strong>Fandong Meng</strong>, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou and Yue Zhang.
<font color="#0080FF">XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners.</font>
In <em> Proceedings of NAACL 2024</em>.
[<a href="https://arxiv.org/abs/2310.05502">arxiv</a>]  
</li>
<li type="circle">
Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, <strong>Fandong Meng</strong>, Mo Yu, Bowen Zhou and Jie Zhou.
<font color="#0080FF">On Large Language Models' Hallucination with Regard to Known Facts.</font>
In <em> Proceedings of NAACL 2024</em>.
[<a href="https://arxiv.org/abs/2403.20009">arxiv</a>]  
</li>
<li type="circle">
Liqiang Niu, <strong>Fandong Meng</strong> and Jie Zhou.
<font color="#0080FF">UMTIT: Unifying Recognition, Translation, and Generation for Multimodal Text Image Translation.</font>
In <em> Proceedings of LREC-COLING 2024</em>.
[<a href="https://aclanthology.org/2024.lrec-main.1474.pdf">paper</a>]  
</li>
<li type="circle">
Jianhao Yan, Jin Xu, <strong>Fandong Meng</strong>, Jie Zhou and Yue Zhang.
<font color="#0080FF">DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding.</font>
In <em> Proceedings of LREC-COLING 2024</em>.
[<a href="https://aclanthology.org/2024.lrec-main.395.pdf">paper</a>]  
</li>
<li type="circle">
Chujie Zheng, Hao Zhou, <strong>Fandong Meng</strong>, Jie Zhou and Minlie Huang.
<font color="#0080FF">Large Language Models Are Not Robust Multiple Choice Selectors.</font>
In <em> Proceedings of ICLR 2024</em>.
[<a href="https://arxiv.org/abs/2309.03882">arxiv</a>]
</li>
<li type="circle">
Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, <strong>Fandong Meng</strong>, Jie Zhou and Xu Sun.
<font color="#0080FF">Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs.</font>
In <em> Proceedings of ICLR 2024</em>.
[<a href="https://arxiv.org/abs/2307.15992">arxiv</a>]
</li>
<li type="circle">
Jiali Zeng, <strong>Fandong Meng</strong>, Yongjing Yin and Jie Zhou.
<font color="#0080FF">Teaching Large Language Models to Translate with Comparison.</font>
In <em> Proceedings of AAAI 2024</em>.
</li>
<li type="circle">
Kun Zhang, Jiali Zeng, <strong>Fandong Meng</strong>, Yuanzhuo Wang, Shiqi Sun, Long Bai, Huawei Shen and Jie Zhou. 
<font color="#0080FF">Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models.</font>
In <em> Proceedings of AAAI 2024</em>.
</li>
<li type="circle">
Xinwei Long, Jiali Zeng, <strong>Fandong Meng</strong>, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou and Jie Zhou.
<font color="#0080FF">Generative Multi-Modal Knowledge Retrieval with Large Language Models.</font>
In <em> Proceedings of AAAI 2024</em>.
[<a href="https://arxiv.org/abs/2401.08206">arxiv</a>]
</li>
</ul> 

        
<h2>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Services</h2>
<ul>
<li type="circle">Action Editor: ACL Rolling Review (2024)</li>
<li type="circle">Standing Reviewer: TACL (2021-)</li>
<li type="circle">Meta-Reviewer: NeurIPS 2024 (Area Chair); AAAI 2022 (Senior PC Member) </li>
<li type="circle">PC Member & Reviewer: NeurIPS (2023); ACL (2014, 2017-2022); EMNLP (2018, 2020-2022); NAACL (2016, 2018, 2021); AAAI (2020, 2021); IJCAI (2019); COLING (2018) </li>
</ul>

<h2>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honours</h2>
<ul>
<li type="circle"><font color=red>Best Long Paper Awards</font> of ACL 2019 and EMNLP 2023. </li> 
<li type="circle">Champion of ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents. </li>
<li type="circle">Champion of WMT22 Chat Translation Task on English->German and German->English. </li>
<li type="circle">Champion of WMT22 Biomedical Translation Task on Chinese->English. </li>  
<li type="circle">Champion of WMT21 news translation task on English->Chinese, English->Japanese, Japanese->English, and English->German (among constrained systems). </li>  
<li type="circle">Champion of WMT20 news translation task on Chinese->English. </li> 
<li type="circle">National Scholarship for Graduate Excellence of UCAS in 2015. </li>
<li type="circle">UCAS Outstanding Student Awards in 2013 and 2014. </li>
<li type="circle">Outstanding Winner (<i>world 1/14</i>) of COMAP's Mathematical Contest in Modeling (MCM/<u>ICM</u>) in 2010. </li>
<li type="circle">First Prize Winner of National Mathematic Contest In Modeling in 2009. </li>
</ul>

      </section>

  </body>
</html>
